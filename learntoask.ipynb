{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc316950-16f5-4ca6-8f18-9a4d7866f79d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d526d5b1-9a5d-4f6d-b819-fbed979113b4",
   "metadata": {},
   "source": [
    "# Steps\n",
    "1. Preprocessing: tokenization, sentence splitting, lower-case, `<SOS>` sentence `<EOS>`\n",
    "2. sentences are in one text file, questions are in another text file\n",
    "3. Vocab: most frequent 45k for sentence, most frequent 28k for questions\n",
    "4. Pretrained Word Embedding: glove.840b.300d in the embedding layer\n",
    "\n",
    "# Architecture Details\n",
    "1. Both encoder & decoder has: Bidirectional LSTM, 2 Layers, 600 hidden dim \n",
    "2. SGD Optimizer\n",
    "3. Initial LR 1.0, at epoch 8, LR 0.5\n",
    "4. batch size 64\n",
    "5. Dropout 0.3 between vertical LSTM stacks\n",
    "6. Gradient Clipping when norm exceeds 5\n",
    "7. Max epoch 15\n",
    "8. Attention based encoding: the encoder produces hidden state from a bidirectional LSTM $h_t$ from input sequence $x$ where $t = 1...|x|$. Attention based encoding $c_t$ is the weighted average of all the hidden states, $h_i$ in that sentence.\n",
    "9. The weight is an attention matrix of shape $(|x|, |x|)$. \\\n",
    "    Attention of i to t, $a_{i,t}$ is exponential of (i's hidden state $\\times$ learnable parameter $\\times$ t's hidden state) over sum of the above for all tokens to t. \n",
    "\n",
    "# Inference\n",
    "1. Beam Size 3\n",
    "2. Generate until `<EOS>` tag found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fb92ad-3bda-41b4-b7d0-13f18ef5e884",
   "metadata": {},
   "source": [
    "I ----------- $h_0$ \\\n",
    "am -----------$h_1$ \\\n",
    "human -------- $h_2$ \\\n",
    "android ------$h_3$ \n",
    "\n",
    "t = 2 \\\n",
    "encoding of `human` = attention `i` to `human` x `i` state \\\n",
    "               \\+ attention `am` to `human` x `am` state\n",
    "\n",
    "thus, encoding of a token is the weighted average of all the token's hidden state in the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0cdea6-8371-4cc7-9254-ef7e69caa92b",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f28a4050-5bbf-4677-b41e-ef8f7ceb75e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import io\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d2cfc4-0e3e-4119-ad96-ad8da9126fa0",
   "metadata": {},
   "source": [
    "# Look at Data\n",
    "Data already preprocessed collected from the original repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73e4df45-47bd-4670-9ddc-c02866e8f9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = Path(\"data/processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "79cc3957-997a-4d87-b6e6-73dfbc7ca441",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a pub / pʌb / , or public house is , despite its name , a private house , but is called a public house because it is licensed to sell alcohol to the general public . \n",
      "\n",
      "\n",
      "what is a pub licensed to sell ?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "a pub / pʌb / , or public house is , despite its name , a private house , but is called a public house because it is licensed to sell alcohol to the general public . \n",
      "\n",
      "\n",
      "what is the term ` pub ' short for ?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "the writings of samuel pepys describe the pub as the heart of england . \n",
      "\n",
      "\n",
      "who said that pubs are the heart of england ?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "the history of pubs can be traced back to roman taverns , through the anglo-saxon alehouse to the development of the modern tied house system in the 19th century . \n",
      "\n",
      "\n",
      "how far back does the history of pubs go back ?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "the history of pubs can be traced back to roman taverns , through the anglo-saxon alehouse to the development of the modern tied house system in the 19th century . \n",
      "\n",
      "\n",
      "what is a pub tied to in the 19th century ?\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_n = 5\n",
    "with open(data_root / \"src-train.txt\") as f:\n",
    "    _sentences = [f.readline() for i in range(_n)]\n",
    "\n",
    "with open(data_root / \"tgt-train.txt\") as f:\n",
    "    _questions = [f.readline() for i in range(_n)]\n",
    "\n",
    "for s, q in zip(_sentences, _questions):\n",
    "    print(s)\n",
    "    print()\n",
    "    print(q)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3dd7c3bc-bffd-4c4b-a699-ca7b1f12c505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70484 70484\n"
     ]
    }
   ],
   "source": [
    "with open(data_root / \"src-train.txt\") as f:\n",
    "    _sentences = f.readlines()\n",
    "\n",
    "with open(data_root / \"tgt-train.txt\") as f:\n",
    "    _questions = f.readlines()\n",
    "\n",
    "print(len(_sentences), len(_questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898e4fe2-49a3-49a7-be64-d097fea0d979",
   "metadata": {},
   "source": [
    "# Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "13368cb1-59d9-46c1-b2d7-8ddfa805eb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.vocab import build_vocab_from_iterator, GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "11f2e760-9b15-421a-83bf-6dd8d4476ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_token(text_file_path):\n",
    "    with io.open(text_file_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            yield line.strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "881a0495-7105-4dd5-9ad2-55ab29bd91a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 850 ms, sys: 92.8 ms, total: 942 ms\n",
      "Wall time: 936 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sentence_vocab = build_vocab_from_iterator(\n",
    "    yield_token(data_root / \"src-train.txt\"),\n",
    "    max_tokens=45000,\n",
    "    specials=[\"<SOS>\", \"<EOS>\", \"<PAD>\", \"<UNK>\"],\n",
    "    special_first=True,\n",
    ")\n",
    "\n",
    "question_vocab = build_vocab_from_iterator(\n",
    "    yield_token(data_root / \"tgt-train.txt\"), max_tokens=28000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ad1693ea-dba6-471c-889f-f5f211578744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49823\n"
     ]
    }
   ],
   "source": [
    "# merge two vocabs once collected from separate corpus\n",
    "vocab = torchtext.vocab.Vocab(sentence_vocab)\n",
    "vocab.set_default_index(vocab[\"<UNK>\"])\n",
    "\n",
    "for token in question_vocab.get_itos():\n",
    "    if token not in vocab:\n",
    "        vocab.append_token(token)\n",
    "\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "57a20090-928c-4c2c-8413-a047c5ba7f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector = torch.zeros(size=(len(vocab), 300))\n",
    "glove = GloVe(cache=\"data/\")\n",
    "for index in range(len(vocab)):\n",
    "    embedding_vector[index] = glove[vocab.lookup_token(index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47cc5def-eaad-4d3f-b4b8-16593b1dd6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_layer = nn.Embedding.from_pretrained(embeddings=embedding_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1d03c80-f071-4efd-8101-dab7fd5ac18c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca6d3706-b5bc-47e3-9e63-2a5875397416",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c798185b-b029-4bd6-b902-d3c078201faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    encoder decoder model only handling a single sample\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_vector, embedding_dim=300, hidden_dim=8, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_vector)\n",
    "        \n",
    "        self.encoder = nn.LSTM(input_size=embedding_dim, \n",
    "                               hidden_size=hidden_dim,\n",
    "                               batch_first=True,\n",
    "                               bidirectional=bidirectional)\n",
    "\n",
    "        # self.decoder = nn.LSTM(input_size=embedding_dim, \n",
    "        #                       hidden_size=hidden_dim, \n",
    "        #                       batch_first=True,\n",
    "        #                       bidirectional=False)\n",
    "\n",
    "        self.decoder_lstm_cell = nn.LSTMCell(input_size=embedding_dim,\n",
    "                                             hidden_size=hidden_dim)\n",
    "\n",
    "        self.attn_layer = nn.Linear(in_features=hidden_dim,\n",
    "                                    out_features=hidden_dim)\n",
    "\n",
    "        self.decoder_linear = nn.Sequential(nn.Linear(2*hidden_dim, 2*hidden_dim),\n",
    "                                            nn.Tanh(),\n",
    "                                           nn.Linear(2*hidden_dim, vocab_size))\n",
    "\n",
    "\n",
    "    def forward(self, source, tgt):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        source: torch.Tensor. \n",
    "        \"\"\"\n",
    "        source = self.embedding(source)\n",
    "        # b (N, L, d), hT & cT (1, N, d), hidden states from the last timestep\n",
    "        b, (hT, cT) = self.encoder(source)\n",
    "        logits = self.decoder(b, tgt)\n",
    "        return logits\n",
    "\n",
    "\n",
    "    def decoder(self, src_states, tgt):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        src_states: torch.Tensor (N, L, d)\n",
    "        \"\"\"\n",
    "        MAX_LENGTH = len(tgt)\n",
    "        # input\n",
    "        next_token = torch.tensor(45000)\n",
    "        logits = []\n",
    "        t = 0\n",
    "        \n",
    "        while t in range(MAX_LENGTH):\n",
    "            embeddings = self.embedding(next_token)\n",
    "            ht, _ = self.decoder_lstm_cell(embeddings)\n",
    "            at = self.attention(src_states, ht)\n",
    "            # print(at.shape, src_states.shape)\n",
    "            ct = (at.view(1, at.shape[0]) @ src_states).squeeze()\n",
    "            decoder_input = torch.cat((ht, ct))\n",
    "            logit = self.decoder_linear(decoder_input)\n",
    "            logits.append(logit)\n",
    "            next_token = tgt[t]\n",
    "            t+=1\n",
    "\n",
    "        return logits\n",
    "    \n",
    "\n",
    "    def attention(self, hs, ht):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        hs: (|x|, d)\n",
    "        ht: (d,)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        at: (|x|,)\n",
    "        \"\"\"\n",
    "        # (|x|, d)\n",
    "        attn = self.attn_layer(hs)\n",
    "        # (1, d) (d, |x|) = (1, |x|)\n",
    "        attn =  ht.T @ attn.T\n",
    "        return attn.T / attn.T.sum(dim=0)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7def868-84a8-4949-8055-f558a7f4a93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "model = EncoderDecoder(len(vocab), embedding_vector)\n",
    "logits =model(_src_indexed, _tgt_indexed)\n",
    "print(len(logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b7ae719d-8276-43fd-b41e-faea95f3bb79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcb017970a0>]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train_epoch(model, optimizer):\n",
    "    criterion = nn.NLLLoss()\n",
    "    total_loss = 0\n",
    "    for data in zip(_sentences, _questions):\n",
    "        input_tensor = torch.tensor(vocab(data[0].split()), dtype=torch.long)\n",
    "        _target = data[1].split()\n",
    "        _target.append('<EOS>')\n",
    "        target_tensor = torch.tensor(vocab(_target), dtype=torch.long)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(input_tensor, target_tensor)\n",
    "        final_logits = torch.cat(logits).view(-1, logits[0].size()[0])\n",
    "        # print(target_tensor.shape, final_logits.shape)\n",
    "\n",
    "        loss = criterion(\n",
    "            final_logits,\n",
    "            target_tensor\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(_questions)\n",
    "\n",
    "model = EncoderDecoder(len(vocab), embedding_vector)\n",
    "optim = torch.optim.SGD(model.parameters(),lr = 1.0)\n",
    "\n",
    "losses = []\n",
    "for i in range(10):\n",
    "    loss = train_epoch(model, optim)\n",
    "    losses.append(loss)\n",
    "\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dac04a8d-f15a-4272-9557-ab99add0bfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "_a = [torch.randn((10, )) for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b3dc6bb5-b67f-44ea-8ef6-716947131a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([-1.0744, -0.3082,  0.8412, -0.7324, -0.6063, -0.0695, -0.0040,  1.6255,\n",
      "        -1.4629,  0.3666]), tensor([-0.0700,  0.3958, -0.5411,  0.7852, -0.8056,  0.6153, -0.3385, -1.3658,\n",
      "        -1.4265, -0.3672]), tensor([ 0.7445,  1.2010, -0.9885, -1.0478, -0.0517,  0.7053, -0.9229,  0.0703,\n",
      "        -1.9221, -1.6314])]\n",
      "tensor([[-1.0744, -0.3082,  0.8412, -0.7324, -0.6063, -0.0695, -0.0040,  1.6255,\n",
      "         -1.4629,  0.3666],\n",
      "        [-0.0700,  0.3958, -0.5411,  0.7852, -0.8056,  0.6153, -0.3385, -1.3658,\n",
      "         -1.4265, -0.3672],\n",
      "        [ 0.7445,  1.2010, -0.9885, -1.0478, -0.0517,  0.7053, -0.9229,  0.0703,\n",
      "         -1.9221, -1.6314]])\n"
     ]
    }
   ],
   "source": [
    "print(_a)\n",
    "_a = torch.cat(_a).view(-1, 10)\n",
    "print(_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59edf048-0115-4d44-995b-44918b98be8c",
   "metadata": {},
   "source": [
    "# Batch Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "023146f9-a3be-48bb-b09f-53192e2494c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"sentence_max_length\": 150, \"question_max_length\": 50, \"batch_size\": 32}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "27491514-0299-481c-9014-23e1a0301f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "70fdf903-14d4-458a-9e6e-5c19f7a3a610",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceQuestionDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab: torchtext.vocab.Vocab,\n",
    "        sentences: List[str],\n",
    "        questions: List[str],\n",
    "        Ls=150,\n",
    "        Lq=50,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Represents a dataset of text pairs for training or evaluating models that\n",
    "        deal with relationships between text passages.\n",
    "\n",
    "        Args:\n",
    "            vocab (torchtext.vocab.Vocab): A pre-built vocabulary object\n",
    "                containing word mappings from text to numerical representation.\n",
    "            sentences (List[str]): A list of text passages (sentences, paragraphs, etc.).\n",
    "            questions (List[str]): A list of corresponding questions related to the sentences.\n",
    "            Ls (int, optional): The maximum length to which sentences will be\n",
    "                truncated or padded during preprocessing (default: 150).\n",
    "            Lq (int, optional): The maximum length to which questions will be\n",
    "                truncated or padded during preprocessing (default: 50).\n",
    "        \"\"\"\n",
    "        pad_index = vocab[\"<PAD>\"]\n",
    "        sos_index = vocab[\"<SOS>\"]\n",
    "        eos_index = vocab[\"<EOS>\"]\n",
    "\n",
    "        sentences_indexed = [vocab(document.split()) for document in sentences]\n",
    "        questions_indexed = [vocab(document.split()) for document in questions]\n",
    "\n",
    "        self.sentences_tensor = torch.full(\n",
    "            (len(sentences), Ls), pad_index, dtype=torch.long\n",
    "        )\n",
    "        self.questions_tensor = torch.full(\n",
    "            (len(questions), Lq), pad_index, dtype=torch.long\n",
    "        )\n",
    "\n",
    "        for i, data in enumerate(zip(sentences_indexed, questions_indexed)):\n",
    "            # clip sentence length to Ls and add <sos>, <eos>\n",
    "\n",
    "            _sentence_indices = (\n",
    "                data[0] if len(data[0]) <= (Ls - 2) else data[0][: (Ls - 2)]\n",
    "            )\n",
    "            _sentence_indices = (\n",
    "                [\n",
    "                    sos_index,\n",
    "                ]\n",
    "                + _sentence_indices\n",
    "                + [\n",
    "                    eos_index,\n",
    "                ]\n",
    "            )\n",
    "            ls = len(_sentence_indices)\n",
    "            self.sentences_tensor[i, :ls] = torch.tensor(_sentence_indices)\n",
    "\n",
    "            # clip question length to Lq and add <sos>, <eos>\n",
    "            _question_indices = (\n",
    "                data[1] if len(data[1]) <= (Lq - 2) else data[1][: (Lq - 2)]\n",
    "            )\n",
    "            _question_indices = (\n",
    "                [\n",
    "                    sos_index,\n",
    "                ]\n",
    "                + _question_indices\n",
    "                + [\n",
    "                    eos_index,\n",
    "                ]\n",
    "            )\n",
    "            # lq may or may not be equal to Lq\n",
    "            lq = len(_question_indices)\n",
    "            self.questions_tensor[i, :lq] = torch.tensor(_question_indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.sentences_tensor.size(0)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.sentences_tensor[index], self.questions_tensor[index])\n",
    "\n",
    "\n",
    "train_ds = SentenceQuestionDataset(\n",
    "    vocab,\n",
    "    _sentences,\n",
    "    _questions,\n",
    "    Ls=config[\"sentence_max_length\"],\n",
    "    Lq=config[\"question_max_length\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cb93a8c4-5982-43d8-af21-98e960b42db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2203\n"
     ]
    }
   ],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "print(len(train_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565a6716-ab24-4fec-a4e7-ee80f9eecd24",
   "metadata": {},
   "source": [
    "# Batched Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d204de4b-5c55-403d-bc06-e6da7a6342b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    encoder decoder model only handling a single sample\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_vector, hidden_dim=8, bidirectional=False):\n",
    "        super().__init__()\n",
    "        embedding_dim = embedding_vector.size(1)\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_vector)\n",
    "\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "\n",
    "        self.decoder_lstm_cell = nn.LSTMCell(\n",
    "            input_size=embedding_dim, hidden_size=hidden_dim\n",
    "        )\n",
    "\n",
    "        self.attn_layer = nn.Linear(in_features=hidden_dim, out_features=hidden_dim)\n",
    "\n",
    "        self.decoder_linear = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_dim, 2 * hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(2 * hidden_dim, vocab_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src (torch.Tensor): (N, Ls) A batch of source sentences represented as tensors.\n",
    "            tgt (torch.Tensor): (N, Lq) A batch of target questions represented as tensors.\n",
    "        \"\"\"\n",
    "        # source => (N, Ls, embedding_dim)\n",
    "        embeddings = self.embedding(src)\n",
    "\n",
    "        # encoder_representation (N, Ls, d), hT => cT => (#direction * #layer, N, d) : hidden states from the last timestep\n",
    "        encoder_repr, (hT, cT) = self.encoder(embeddings)\n",
    "\n",
    "        # (N, Lq, vocab_size)\n",
    "        logits = self.decoder(encoder_repr, tgt, hT, cT)\n",
    "        return logits\n",
    "\n",
    "    def decoder(self, encoder_repr, tgt, hidden_state, cell_state):\n",
    "        hidden_state = hidden_state.squeeze()\n",
    "        cell_state = cell_state.squeeze()\n",
    "        MAX_LENGTH = tgt.size(1)\n",
    "        logits = []\n",
    "\n",
    "        for t in range(MAX_LENGTH):\n",
    "            # => (N,)\n",
    "            next_token = tgt[:, t]\n",
    "            # => (N, embedding_dim)\n",
    "            embeddings = self.embedding(next_token)\n",
    "            # both => (N, d)\n",
    "            hidden_state, cell_state = self.decoder_lstm_cell(\n",
    "                embeddings, (hidden_state, cell_state)\n",
    "            )\n",
    "            # => (N, Ls)\n",
    "            attn = self.attention(encoder_repr, hidden_state)\n",
    "            # (N, 1, Ls) . (N, Ls, d) => (N, d)\n",
    "            attn_based_ctx = torch.bmm(attn.unsqueeze(dim=1), encoder_repr).squeeze()\n",
    "            # => (N, d*2)\n",
    "            decoder_input = torch.cat((hidden_state, attn_based_ctx), dim=1)\n",
    "            # => (N, vocab_size)\n",
    "            logit = self.decoder_linear(decoder_input)\n",
    "            logits.append(logit)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def attention(self, encoder_repr, decoder_hidden_state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            encoder_repr (torch.Tensor): Encoder hidden states of shape (N, Ls, d).\n",
    "            decoder_hidden_state (torch.Tensor): Decoder hidden state at timestep t of shape (N, d)\n",
    "        \"\"\"\n",
    "\n",
    "        # => (N, Ls, d)\n",
    "        attn = self.attn_layer(encoder_repr)\n",
    "        # (N, d) (N, d, Ls) = (N, Ls)\n",
    "        attn = decoder_hidden_state.unsqueeze(dim=1) @ attn.transpose(1, 2)\n",
    "        attn = attn.squeeze()\n",
    "        return attn / attn.sum(dim=1).unsqueeze(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b79ab52a-d0ab-40dd-bce8-4b1e77599eeb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[123], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 26\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m, i, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     28\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "Cell \u001b[0;32mIn[123], line 8\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer)\u001b[0m\n\u001b[1;32m      5\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      7\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(data[\u001b[38;5;241m0\u001b[39m], data[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m----> 8\u001b[0m final_logits \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\n\u001b[1;32m      9\u001b[0m     logits[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, logits[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(final_logits\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m), data[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_epoch(model, optimizer):\n",
    "    criterion = nn.NLLLoss()\n",
    "    total_loss = 0\n",
    "    for data in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(data[0], data[1])\n",
    "        final_logits = torch.cat(logits, dim=0).view(\n",
    "            logits[0].size(0), -1, logits[0].size(1)\n",
    "        )\n",
    "        loss = criterion(final_logits.transpose(1, 2), data[1])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(train_dl)\n",
    "\n",
    "\n",
    "model = EncoderDecoder(len(vocab), embedding_vector)\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1.0)\n",
    "\n",
    "losses = []\n",
    "for i in range(10):\n",
    "    loss = train_epoch(model, optim)\n",
    "    print(\"epoch\", i, \"loss\", loss.item())\n",
    "    losses.append(loss)\n",
    "\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dcbe2ca5-9664-49e1-9b51-dd78227da610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
