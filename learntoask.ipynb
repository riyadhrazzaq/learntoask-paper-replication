{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d526d5b1-9a5d-4f6d-b819-fbed979113b4",
   "metadata": {},
   "source": [
    "# Steps\n",
    "1. Preprocessing: tokenization, sentence splitting, lower-case, `<SOS>` sentence `<EOS>`\n",
    "2. sentences are in one text file, questions are in another text file\n",
    "3. Vocab: most frequent 45k for sentence, most frequent 28k for questions\n",
    "4. Pretrained Word Embedding: glove.840b.300d in the embedding layer\n",
    "\n",
    "# Architecture Details\n",
    "1. Both encoder & decoder has: Bidirectional LSTM, 2 Layers, 600 hidden dim \n",
    "2. SGD Optimizer\n",
    "3. Initial LR 1.0, at epoch 8, LR 0.5\n",
    "4. batch size 64\n",
    "5. Dropout 0.3 between vertical LSTM stacks\n",
    "6. Gradient Clipping when norm exceeds 5\n",
    "7. Max epoch 15\n",
    "8. Attention based encoding: the encoder produces hidden state from a bidirectional LSTM $h_t$ from input sequence $x$ where $t = 1...|x|$. Attention based encoding $c_t$ is the weighted average of all the hidden states, $h_i$ in that sentence.\n",
    "9. The weight is an attention matrix of shape $(|x|, |x|)$. \\\n",
    "    Attention of i to t, $a_{i,t}$ is exponential of (i's hidden state $\\times$ learnable parameter $\\times$ t's hidden state) over sum of the above for all tokens to t. \n",
    "\n",
    "# Inference\n",
    "1. Beam Size 3\n",
    "2. Generate until `<EOS>` tag found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fb92ad-3bda-41b4-b7d0-13f18ef5e884",
   "metadata": {},
   "source": [
    "I ----------- $h_0$ \\\n",
    "am -----------$h_1$ \\\n",
    "human -------- $h_2$ \\\n",
    "android ------$h_3$ \n",
    "\n",
    "t = 2 \\\n",
    "encoding of `human` = attention `i` to `human` x `i` state \\\n",
    "               \\+ attention `am` to `human` x `am` state\n",
    "\n",
    "thus, encoding of a token is the weighted average of all the token's hidden state in the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0cdea6-8371-4cc7-9254-ef7e69caa92b",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f28a4050-5bbf-4677-b41e-ef8f7ceb75e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import io\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d2cfc4-0e3e-4119-ad96-ad8da9126fa0",
   "metadata": {},
   "source": [
    "# Look at Data\n",
    "Data already preprocessed collected from the original repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73e4df45-47bd-4670-9ddc-c02866e8f9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = Path('data/processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79cc3957-997a-4d87-b6e6-73dfbc7ca441",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a pub / pʌb / , or public house is , despite its name , a private house , but is called a public house because it is licensed to sell alcohol to the general public . \n",
      "\n",
      "\n",
      "what is a pub licensed to sell ?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "a pub / pʌb / , or public house is , despite its name , a private house , but is called a public house because it is licensed to sell alcohol to the general public . \n",
      "\n",
      "\n",
      "what is the term ` pub ' short for ?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "the writings of samuel pepys describe the pub as the heart of england . \n",
      "\n",
      "\n",
      "who said that pubs are the heart of england ?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "the history of pubs can be traced back to roman taverns , through the anglo-saxon alehouse to the development of the modern tied house system in the 19th century . \n",
      "\n",
      "\n",
      "how far back does the history of pubs go back ?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "the history of pubs can be traced back to roman taverns , through the anglo-saxon alehouse to the development of the modern tied house system in the 19th century . \n",
      "\n",
      "\n",
      "what is a pub tied to in the 19th century ?\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_n = 5\n",
    "with open(data_root / 'src-train.txt') as f:\n",
    "    _sentences = [f.readline() for i in range(_n)]\n",
    "\n",
    "with open(data_root / 'tgt-train.txt') as f:\n",
    "    _questions = [f.readline() for i in range(_n)]\n",
    "\n",
    "for s, q in zip(_sentences, _questions):\n",
    "    print(s)\n",
    "    print()\n",
    "    print(q)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898e4fe2-49a3-49a7-be64-d097fea0d979",
   "metadata": {},
   "source": [
    "# Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "13368cb1-59d9-46c1-b2d7-8ddfa805eb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.vocab import build_vocab_from_iterator, GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11f2e760-9b15-421a-83bf-6dd8d4476ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_token(text_file_path):\n",
    "    with io.open(text_file_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            yield line.strip().split()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "881a0495-7105-4dd5-9ad2-55ab29bd91a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 812 ms, sys: 82.7 ms, total: 895 ms\n",
      "Wall time: 910 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sentence_vocab = build_vocab_from_iterator(yield_token(data_root / 'src-train.txt'), \n",
    "                                           max_tokens=45000)\n",
    "question_vocab = build_vocab_from_iterator(yield_token(data_root / 'tgt-train.txt'), \n",
    "                                           max_tokens=28000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ad1693ea-dba6-471c-889f-f5f211578744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49825\n"
     ]
    }
   ],
   "source": [
    "# merge two vocabs once collected from separate corpus\n",
    "vocab = torchtext.vocab.Vocab(sentence_vocab)\n",
    "\n",
    "vocab.append_token('<SOS>')\n",
    "vocab.append_token('<EOS>')\n",
    "vocab.append_token('<UNK>')\n",
    "vocab.set_default_index(vocab['<UNK>'])\n",
    "\n",
    "for token in question_vocab.get_itos():\n",
    "    if token not in vocab:\n",
    "        vocab.append_token(token)\n",
    "\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "57a20090-928c-4c2c-8413-a047c5ba7f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector = torch.zeros(size=(len(vocab), 300))\n",
    "glove = GloVe(cache=\"data/\")\n",
    "for index in range(len(vocab)):\n",
    "    embedding_vector[index] = glove[vocab.lookup_token(index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "47cc5def-eaad-4d3f-b4b8-16593b1dd6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_layer = nn.Embedding.from_pretrained(embeddings=embedding_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6d3706-b5bc-47e3-9e63-2a5875397416",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce85270-b61a-47f4-9460-ed22b680ad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim=600):\n",
    "        self.hidden_dim = 600\n",
    "        self.linear = nn.Linear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d4eda846-4911-418d-9ecc-30bcd9555afc",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3192411227.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[65], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    assert vocab is not None,\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class LearnToAsk(nn.Module):\n",
    "    def __init__(self, embedding_vector=None, vocab=None):\n",
    "        if embedding_vector:\n",
    "            self.embedding_layer = nn.Embedding.from_pretrained(embeddings=embedding_vector)\n",
    "        else:\n",
    "            assert vocab is not None, \"provide either embedding vector or vocab\"\n",
    "            self.embedding_layer = nn.Embedding(len(vocab), 300)\n",
    "\n",
    "        self."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875f2d56-4dba-439a-8951-ad125c999f07",
   "metadata": {},
   "source": [
    "## Figuring out Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c798185b-b029-4bd6-b902-d3c078201faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding input torch.Size([2, 4, 5])\n",
      "hidden states torch.Size([2, 4, 12])\n"
     ]
    }
   ],
   "source": [
    "corpus = ['the brown fox jumped', 'have a little faith']\n",
    "embedding_dim = 5\n",
    "embedding = torch.randn(size=(2, 4, embedding_dim))  # embedding dim = 5\n",
    "lstm = nn.LSTM(input_size=embedding_dim,\n",
    "               hidden_size=6,\n",
    "               num_layers=2,\n",
    "               batch_first=True,\n",
    "               dropout=0.3,\n",
    "               bidirectional=True)\n",
    "\n",
    "output, _ = lstm(embedding)\n",
    "print('embedding input', embedding.shape)\n",
    "print('hidden states', output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bea5c7-902a-4771-b050-d32649e02d4a",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"data/attention.png\" width=\"300\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029694a1-1d43-4d7d-81c2-f555f5ea5559",
   "metadata": {},
   "source": [
    "Consider a single sample, ht = (12, 1), htT = (1, 12), bi = (12, 1), ait is a float. Wb @ biT must produce a (12, 1), so Wb = (12, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fb3e17-555f-42bc-a105-ac94ec4e65a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
