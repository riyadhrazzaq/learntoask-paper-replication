{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d526d5b1-9a5d-4f6d-b819-fbed979113b4",
   "metadata": {},
   "source": [
    "# Steps\n",
    "1. Preprocessing: tokenization, sentence splitting, lower-case, `<SOS>` sentence `<EOS>`\n",
    "2. sentences are in one text file, questions are in another text file\n",
    "3. Vocab: most frequent 45k for sentence, most frequent 28k for questions\n",
    "4. Pretrained Word Embedding: glove.840b.300d in the embedding layer\n",
    "\n",
    "# Architecture Details\n",
    "1. Both encoder & decoder has: Bidirectional LSTM, 2 Layers, 600 hidden dim \n",
    "2. SGD Optimizer\n",
    "3. Initial LR 1.0, at epoch 8, LR 0.5\n",
    "4. batch size 64\n",
    "5. Dropout 0.3 between vertical LSTM stacks\n",
    "6. Gradient Clipping when norm exceeds 5\n",
    "7. Max epoch 15\n",
    "8. Attention based encoding: the encoder produces hidden state from a bidirectional LSTM $h_t$ from input sequence $x$ where $t = 1...|x|$. Attention based encoding $c_t$ is the weighted average of all the hidden states, $h_i$ in that sentence.\n",
    "9. The weight is an attention matrix of shape $(|x|, |x|)$. \\\n",
    "    Attention of i to t, $a_{i,t}$ is exponential of (i's hidden state $\\times$ learnable parameter $\\times$ t's hidden state) over sum of the above for all tokens to t. \n",
    "\n",
    "# Inference\n",
    "1. Beam Size 3\n",
    "2. Generate until `<EOS>` tag found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fb92ad-3bda-41b4-b7d0-13f18ef5e884",
   "metadata": {},
   "source": [
    "I ----------- $h_0$ \\\n",
    "am -----------$h_1$ \\\n",
    "human -------- $h_2$ \\\n",
    "android ------$h_3$ \n",
    "\n",
    "t = 2 \\\n",
    "encoding of `human` = attention `i` to `human` x `i` state \\\n",
    "               \\+ attention `am` to `human` x `am` state\n",
    "\n",
    "thus, encoding of a token is the weighted average of all the token's hidden state in the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0cdea6-8371-4cc7-9254-ef7e69caa92b",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f28a4050-5bbf-4677-b41e-ef8f7ceb75e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import io\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d2cfc4-0e3e-4119-ad96-ad8da9126fa0",
   "metadata": {},
   "source": [
    "# Look at Data\n",
    "Data already preprocessed collected from the original repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73e4df45-47bd-4670-9ddc-c02866e8f9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = Path('data/processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79cc3957-997a-4d87-b6e6-73dfbc7ca441",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a pub / pʌb / , or public house is , despite its name , a private house , but is called a public house because it is licensed to sell alcohol to the general public . \n",
      "\n",
      "\n",
      "what is a pub licensed to sell ?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "a pub / pʌb / , or public house is , despite its name , a private house , but is called a public house because it is licensed to sell alcohol to the general public . \n",
      "\n",
      "\n",
      "what is the term ` pub ' short for ?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "the writings of samuel pepys describe the pub as the heart of england . \n",
      "\n",
      "\n",
      "who said that pubs are the heart of england ?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "the history of pubs can be traced back to roman taverns , through the anglo-saxon alehouse to the development of the modern tied house system in the 19th century . \n",
      "\n",
      "\n",
      "how far back does the history of pubs go back ?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "the history of pubs can be traced back to roman taverns , through the anglo-saxon alehouse to the development of the modern tied house system in the 19th century . \n",
      "\n",
      "\n",
      "what is a pub tied to in the 19th century ?\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_n = 5\n",
    "with open(data_root / 'src-train.txt') as f:\n",
    "    _sentences = [f.readline() for i in range(_n)]\n",
    "\n",
    "with open(data_root / 'tgt-train.txt') as f:\n",
    "    _questions = [f.readline() for i in range(_n)]\n",
    "\n",
    "for s, q in zip(_sentences, _questions):\n",
    "    print(s)\n",
    "    print()\n",
    "    print(q)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898e4fe2-49a3-49a7-be64-d097fea0d979",
   "metadata": {},
   "source": [
    "# Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13368cb1-59d9-46c1-b2d7-8ddfa805eb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.vocab import build_vocab_from_iterator, GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11f2e760-9b15-421a-83bf-6dd8d4476ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_token(text_file_path):\n",
    "    with io.open(text_file_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            yield line.strip().split()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "881a0495-7105-4dd5-9ad2-55ab29bd91a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 801 ms, sys: 100 ms, total: 901 ms\n",
      "Wall time: 905 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sentence_vocab = build_vocab_from_iterator(yield_token(data_root / 'src-train.txt'), \n",
    "                                           max_tokens=45000)\n",
    "question_vocab = build_vocab_from_iterator(yield_token(data_root / 'tgt-train.txt'), \n",
    "                                           max_tokens=28000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad1693ea-dba6-471c-889f-f5f211578744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49825\n"
     ]
    }
   ],
   "source": [
    "# merge two vocabs once collected from separate corpus\n",
    "vocab = torchtext.vocab.Vocab(sentence_vocab)\n",
    "\n",
    "vocab.append_token('<SOS>')\n",
    "vocab.append_token('<EOS>')\n",
    "vocab.append_token('<UNK>')\n",
    "vocab.set_default_index(vocab['<UNK>'])\n",
    "\n",
    "for token in question_vocab.get_itos():\n",
    "    if token not in vocab:\n",
    "        vocab.append_token(token)\n",
    "\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57a20090-928c-4c2c-8413-a047c5ba7f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector = torch.zeros(size=(len(vocab), 300))\n",
    "glove = GloVe(cache=\"data/\")\n",
    "for index in range(len(vocab)):\n",
    "    embedding_vector[index] = glove[vocab.lookup_token(index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47cc5def-eaad-4d3f-b4b8-16593b1dd6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_layer = nn.Embedding.from_pretrained(embeddings=embedding_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1d03c80-f071-4efd-8101-dab7fd5ac18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,  1840,  3326, 45002],\n",
      "        [   36,     7,   771,  1298]])\n"
     ]
    }
   ],
   "source": [
    "corpus = ['the brown fox jumped', 'have a little faith']\n",
    "_indexed = [vocab(document.split()) for document in corpus]\n",
    "indexed = torch.tensor(_indexed, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6d3706-b5bc-47e3-9e63-2a5875397416",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c798185b-b029-4bd6-b902-d3c078201faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, embedding_vector, embedding_dim=300, hidden_dim=8, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_vector)\n",
    "        \n",
    "        self.encoder = nn.LSTM(input_size=embedding_dim, \n",
    "                               hidden_size=hidden_dim,\n",
    "                               batch_first=True,\n",
    "                               bidirectional=bidirectional)\n",
    "\n",
    "        # self.decoder = nn.LSTM(input_size=embedding_dim, \n",
    "        #                       hidden_size=hidden_dim, \n",
    "        #                       batch_first=True,\n",
    "        #                       bidirectional=False)\n",
    "\n",
    "        self.decoder_lstm_cell = nn.LSTMCell(input_size=embedding_dim,\n",
    "                                             hidden_size=hidden_dim)\n",
    "\n",
    "        self.attn_linear = nn.Linear(in_features=hidden_dim,\n",
    "                                    out_features=hidden_dim)\n",
    "\n",
    "\n",
    "    def forward(self, source):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        source: torch.Tensor. \n",
    "        \"\"\"\n",
    "        source = self.embedding(source)\n",
    "        # b (N, L, d), hT & cT (1, N, d), hidden states from the last timestep\n",
    "        b, (hT, cT) = self.encoder(source)\n",
    "        h = self.decoder(b)\n",
    "        print(h.shape)\n",
    "\n",
    "\n",
    "    def decoder(self, src_states):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        src_states: torch.Tensor (N, L, d)\n",
    "        \"\"\"\n",
    "        prefix = torch.tensor([[45000,],[45000,]])\n",
    "        t = 0\n",
    "        \n",
    "        while True:\n",
    "            embeddings = self.embedding(prefix)\n",
    "            ht, _ = self.decoder_lstm_cell(embeddings[:, t, :])\n",
    "            at = self.attention(src_states, ht)\n",
    "            # max step\n",
    "            t+=1\n",
    "            if t > 5: break\n",
    "\n",
    "    def attention(self, src_states, ht):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b7def868-84a8-4949-8055-f558a7f4a93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 300]) torch.Size([2, 300])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m EncoderDecoder(embedding_vector)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/codes/nlp/learn2ask/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/codes/nlp/learn2ask/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[46], line 32\u001b[0m, in \u001b[0;36mEncoderDecoder.forward\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# b (N, L, d), hT & cT (1, N, d), hidden states from the last timestep\u001b[39;00m\n\u001b[1;32m     31\u001b[0m b, (hT, cT) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(source)\n\u001b[0;32m---> 32\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(h\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[46], line 47\u001b[0m, in \u001b[0;36mEncoderDecoder.decoder\u001b[0;34m(self, src_states)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(prefix)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(embeddings\u001b[38;5;241m.\u001b[39mshape, \u001b[43membeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     48\u001b[0m     ht, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_lstm_cell(embeddings[:, t, :])\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# max step\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 1 with size 1"
     ]
    }
   ],
   "source": [
    "model = EncoderDecoder(embedding_vector)\n",
    "model(indexed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bea5c7-902a-4771-b050-d32649e02d4a",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"data/attention.png\" width=\"300\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029694a1-1d43-4d7d-81c2-f555f5ea5559",
   "metadata": {},
   "source": [
    "Consider a single sample, ht = (12, 1), htT = (1, 12), bi = (12, 1), ait is a float. Wb @ biT must produce a (12, 1), so Wb = (12, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfec946c-742b-4dd9-a1f1-bb719f5dcd44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
