{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e2725cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T17:15:44.611778314Z",
     "start_time": "2024-05-05T17:15:44.534540328Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "# %load_ext jupyter_black\n",
    "\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.trainutil import *\n",
    "from src.metrics import *\n",
    "\n",
    "import yaml\n",
    "\n",
    "data_root = \"../data\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fbf6a2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T17:23:13.842710556Z",
     "start_time": "2024-05-05T17:23:13.774979910Z"
    }
   },
   "outputs": [],
   "source": [
    "experiment_dir = Path(\"../checkpoints/paper\")\n",
    "with open(experiment_dir / \"history/config.yaml\", \"r\") as stream:\n",
    "    cfg = yaml.safe_load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afb0010d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T17:23:14.751476Z",
     "start_time": "2024-05-05T17:23:14.522733490Z"
    }
   },
   "outputs": [],
   "source": [
    "src_tokenizer = torch.load(experiment_dir / \"src_tokenizer.pt\")\n",
    "tgt_tokenizer = torch.load(experiment_dir / \"tgt_tokenizer.pt\")\n",
    "src_vocab = src_tokenizer.vocab\n",
    "tgt_vocab = tgt_tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5bf4826",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T17:23:15.636753437Z",
     "start_time": "2024-05-05T17:23:15.032062213Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Seq2Seq(\n  (src_embedding): Embedding(45000, 300)\n  (tgt_embedding): Embedding(28000, 300)\n  (encoder): Encoder(\n    (embedding): Embedding(45000, 300)\n    (layers): Sequential(\n      (0): Embedding(45000, 300)\n      (1): LSTM(300, 600, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n    )\n  )\n  (decoder): Decoder(\n    (embedding): Embedding(28000, 300)\n    (lstm): LSTM(300, 600, num_layers=2, batch_first=True, dropout=0.3)\n    (attention): Attention(\n      (projection_layer): Linear(in_features=1200, out_features=600, bias=True)\n    )\n    (decoder_linear): Sequential(\n      (0): Linear(in_features=1800, out_features=600, bias=True)\n      (1): Tanh()\n      (2): Linear(in_features=600, out_features=28000, bias=True)\n    )\n  )\n)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Seq2Seq(\n",
    "    src_vocab_size=len(src_vocab),\n",
    "    tgt_vocab_size=len(tgt_vocab),\n",
    "    src_embedding_vector=None,\n",
    "    tgt_embedding_vector=None,\n",
    "    tgt_pad_index=tgt_vocab[\"<PAD>\"],\n",
    "    tgt_sos_index=tgt_vocab[\"<SOS>\"],\n",
    "    tgt_eos_index=tgt_vocab[\"<EOS>\"],\n",
    "    hidden_size=cfg[\"hidden_size\"],\n",
    "    bidirectional=cfg[\"bidirectional\"],\n",
    "    num_layers=cfg[\"num_layers\"],\n",
    "    src_embedding_size=cfg[\"src_embedding_size\"],\n",
    "    tgt_embedding_size=cfg[\"tgt_embedding_size\"],\n",
    "    dropout=cfg[\"dropout\"],\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a8a0c04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T17:23:16.752686657Z",
     "start_time": "2024-05-05T17:23:16.396061104Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 19:23:16,705 ðŸŽ‰ Loaded existing model. Epoch: 13\n"
     ]
    }
   ],
   "source": [
    "model, _, _, epoch = load_checkpoint(model, experiment_dir / \"model_best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b25135fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T17:57:55.244984604Z",
     "start_time": "2024-05-05T17:54:00.724825745Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [03:50<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14min 34s, sys: 2.39 s, total: 14min 36s\n",
      "Wall time: 3min 54s\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'bleu1': 24.942392110824585,\n 'bleu2': 12.529674172401428,\n 'bleu3': 6.909433007240295,\n 'bleu4': 3.9719369262456894,\n 'rougeL': 22.65859991312027}"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "with open(f\"{data_root}/dev.src\") as srcfile:\n",
    "    sources = srcfile.readlines()\n",
    "\n",
    "with open(f\"{data_root}/dev.tgt\") as tgtfile:\n",
    "    references = tgtfile.readlines()\n",
    "\n",
    "\n",
    "hypotheses = []\n",
    "for source in tqdm(sources):\n",
    "    hyp, _ = generate(model, source, src_tokenizer, tgt_tokenizer, cfg, method=\"greedy\")\n",
    "    hypotheses.append(hyp[0])\n",
    "\n",
    "metrics = compute_metrics(hypotheses, references)\n",
    "metrics = {k: v * 100 for k, v in metrics.items()}\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 / 10570\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[0;32m<timed exec>:13\u001B[0m\n",
      "File \u001B[0;32m~/codes/nlp/learningtoask-final/notebooks/../src/modelutil.py:431\u001B[0m, in \u001B[0;36mgenerate\u001B[0;34m(model, sentence, src_tokenizer, tgt_tokenizer, cfg, method, p)\u001B[0m\n\u001B[1;32m    428\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tokens, attention_scores\n\u001B[1;32m    430\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m method \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnucleus\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 431\u001B[0m     tgt_token_ids, attention_scores \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnucleus_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    432\u001B[0m \u001B[43m        \u001B[49m\u001B[43msrc_token_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcfg\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtgt_max_seq\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mp\u001B[49m\n\u001B[1;32m    433\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    434\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tgt_tokenizer\u001B[38;5;241m.\u001B[39mdecode(tgt_token_ids, keep_specials\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m), attention_scores\n",
      "File \u001B[0;32m~/codes/nlp/learningtoask-final/notebooks/../src/modelutil.py:251\u001B[0m, in \u001B[0;36mSeq2Seq.nucleus_generate\u001B[0;34m(self, sources, source_masks, max_seq, p)\u001B[0m\n\u001B[1;32m    246\u001B[0m logit, (h, c), score \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder(\n\u001B[1;32m    247\u001B[0m     encoder_out, decoder_input, h, c, source_masks\n\u001B[1;32m    248\u001B[0m )\n\u001B[1;32m    250\u001B[0m \u001B[38;5;66;03m# (N, 1)\u001B[39;00m\n\u001B[0;32m--> 251\u001B[0m token_ids, token_probs \u001B[38;5;241m=\u001B[39m \u001B[43mnucleus_sample\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlogit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mp\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    253\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, token_id \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(token_ids):\n\u001B[1;32m    254\u001B[0m     output_token_ids[i]\u001B[38;5;241m.\u001B[39mappend(token_id\u001B[38;5;241m.\u001B[39mitem())\n",
      "File \u001B[0;32m~/codes/nlp/learningtoask-final/notebooks/../src/modelutil.py:377\u001B[0m, in \u001B[0;36mnucleus_sample\u001B[0;34m(logits, p)\u001B[0m\n\u001B[1;32m    374\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m logits\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexpected a matrix (batch, vocab_size)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    376\u001B[0m probs \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39msoftmax(logits, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m--> 377\u001B[0m sorted_probs, sorted_indices \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msort\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprobs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdescending\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    378\u001B[0m cumulative_sum \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcumsum(sorted_probs, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    379\u001B[0m out_of_nucleus \u001B[38;5;241m=\u001B[39m cumulative_sum \u001B[38;5;241m>\u001B[39m p\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(f\"{data_root}/dev.src\") as srcfile:\n",
    "    sources = srcfile.readlines()\n",
    "\n",
    "with open(f\"{data_root}/dev.tgt\") as tgtfile:\n",
    "    references = tgtfile.readlines()\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "hypotheses = []\n",
    "\n",
    "i = 0\n",
    "while i < len(sources):\n",
    "    hyp, _ = generate(model, sources[i: i+batch_size], src_tokenizer, tgt_tokenizer, cfg, method=\"nucleus\", p=0.45)\n",
    "    hypotheses.extend(hyp)\n",
    "    i += batch_size\n",
    "    print(f\"{i} / {len(sources)}\", end=\"\\r\")\n",
    "\n",
    "metrics = compute_metrics(hypotheses, references)\n",
    "metrics = {k: v * 100 for k, v in metrics.items()}\n",
    "metrics"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T18:36:52.185518099Z",
     "start_time": "2024-05-05T18:36:48.712482896Z"
    }
   },
   "id": "151f1cb7700a8015"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "with open(f\"{data_root}/dev.src\") as srcfile:\n",
    "    sources = srcfile.readlines()\n",
    "\n",
    "with open(f\"{data_root}/dev.tgt\") as tgtfile:\n",
    "    references = tgtfile.readlines()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T18:08:48.064830140Z",
     "start_time": "2024-05-05T18:08:47.985781455Z"
    }
   },
   "id": "d0ce3f9eef79a484"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this action was upheld because , according to the u.s. court of appeals for the first circuit , her statement suggested a lack of remorse , an attempt to avoid responsibility for her actions , and even a likelihood of repeating her illegal actions . \n",
      "\n",
      "\n",
      "why is giving a defiant speech sometimes more harmful for the individual ?\n",
      "\n",
      "nucleus: why who critics was what before when while what what why what\n",
      "\n",
      "beam: ['why was the name supreme of the united give the us']\n",
      "\n",
      "greedy: ['why was the action of the first debate for queen victoria']\n"
     ]
    }
   ],
   "source": [
    "idx = random.randint(0, 10000)\n",
    "# nucleus\n",
    "hyp_nucleus, _ = generate(model, sources[idx: idx+2], src_tokenizer, tgt_tokenizer, cfg, method=\"nucleus\", p=1)\n",
    "\n",
    "hyp_greedy, _ = generate(model, sources[idx], src_tokenizer, tgt_tokenizer, cfg, method=\"greedy\")\n",
    "\n",
    "hyp_beam, _ = generate(model, sources[idx], src_tokenizer, tgt_tokenizer, cfg, method=\"beam\")\n",
    "\n",
    "\n",
    "print(sources[idx], references[idx], sep='\\n\\n')\n",
    "print(f\"nucleus: {hyp_nucleus[0]}\\n\\nbeam: {hyp_beam}\\n\\ngreedy: {hyp_greedy}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T18:37:16.280142679Z",
     "start_time": "2024-05-05T18:37:15.410785935Z"
    }
   },
   "id": "ebb034fa9c59b937"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
