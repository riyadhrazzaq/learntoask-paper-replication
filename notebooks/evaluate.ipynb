{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e2725cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T17:22:53.787090409Z",
     "start_time": "2024-05-06T17:22:51.314022492Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riyadh/codes/nlp/learningtoask-final/venv/lib64/python3.10/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/riyadh/codes/nlp/learningtoask-final/venv/lib64/python3.10/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "# %load_ext jupyter_black\n",
    "\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.trainutil import *\n",
    "from src.metrics import *\n",
    "\n",
    "import yaml\n",
    "\n",
    "data_root = \"../data\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fbf6a2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T17:22:53.833097150Z",
     "start_time": "2024-05-06T17:22:53.786984293Z"
    }
   },
   "outputs": [],
   "source": [
    "experiment_dir = Path(\"../checkpoints/paper\")\n",
    "with open(experiment_dir / \"history/config.yaml\", \"r\") as stream:\n",
    "    cfg = yaml.safe_load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afb0010d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T17:22:53.970852783Z",
     "start_time": "2024-05-06T17:22:53.806377420Z"
    }
   },
   "outputs": [],
   "source": [
    "src_tokenizer = torch.load(experiment_dir / \"src_tokenizer.pt\")\n",
    "tgt_tokenizer = torch.load(experiment_dir / \"tgt_tokenizer.pt\")\n",
    "src_vocab = src_tokenizer.vocab\n",
    "tgt_vocab = tgt_tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5bf4826",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T17:22:54.352633703Z",
     "start_time": "2024-05-06T17:22:53.973547669Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Seq2Seq(\n  (src_embedding): Embedding(45000, 300)\n  (tgt_embedding): Embedding(28000, 300)\n  (encoder): Encoder(\n    (embedding): Embedding(45000, 300)\n    (layers): Sequential(\n      (0): Embedding(45000, 300)\n      (1): LSTM(300, 600, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n    )\n  )\n  (decoder): Decoder(\n    (embedding): Embedding(28000, 300)\n    (lstm): LSTM(300, 600, num_layers=2, batch_first=True, dropout=0.3)\n    (attention): Attention(\n      (projection_layer): Linear(in_features=1200, out_features=600, bias=True)\n    )\n    (decoder_linear): Sequential(\n      (0): Linear(in_features=1800, out_features=600, bias=True)\n      (1): Tanh()\n      (2): Linear(in_features=600, out_features=28000, bias=True)\n    )\n  )\n)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Seq2Seq(\n",
    "    src_vocab_size=len(src_vocab),\n",
    "    tgt_vocab_size=len(tgt_vocab),\n",
    "    src_embedding_vector=None,\n",
    "    tgt_embedding_vector=None,\n",
    "    tgt_pad_index=tgt_vocab[\"<PAD>\"],\n",
    "    tgt_sos_index=tgt_vocab[\"<SOS>\"],\n",
    "    tgt_eos_index=tgt_vocab[\"<EOS>\"],\n",
    "    hidden_size=cfg[\"hidden_size\"],\n",
    "    bidirectional=cfg[\"bidirectional\"],\n",
    "    num_layers=cfg[\"num_layers\"],\n",
    "    src_embedding_size=cfg[\"src_embedding_size\"],\n",
    "    tgt_embedding_size=cfg[\"tgt_embedding_size\"],\n",
    "    dropout=cfg[\"dropout\"],\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a8a0c04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T17:22:54.721090021Z",
     "start_time": "2024-05-06T17:22:54.352347443Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-06 19:22:54,663 ðŸŽ‰ Loaded existing model. Epoch: 13\n"
     ]
    }
   ],
   "source": [
    "model, _, _, epoch = load_checkpoint(model, experiment_dir / \"model_best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b25135fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T17:23:05.352260124Z",
     "start_time": "2024-05-06T17:22:54.700093128Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 61/10570 [00:10<29:03,  6.03it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[0;32m<timed exec>:10\u001B[0m\n",
      "File \u001B[0;32m~/codes/nlp/learningtoask-final/notebooks/../src/modelutil.py:456\u001B[0m, in \u001B[0;36mgenerate\u001B[0;34m(model, sentence, src_tokenizer, tgt_tokenizer, cfg, method, p)\u001B[0m\n\u001B[1;32m    454\u001B[0m attention_scores \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m method \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgreedy\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 456\u001B[0m     tgt_token_ids, attention_scores \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgreedy_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    457\u001B[0m \u001B[43m        \u001B[49m\u001B[43msrc_token_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcfg\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtgt_max_seq\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop_at_eos\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[1;32m    458\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    459\u001B[0m     attention_scores \u001B[38;5;241m=\u001B[39m attention_scores\u001B[38;5;241m.\u001B[39msqueeze(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m)\n\u001B[1;32m    460\u001B[0m     tokens \u001B[38;5;241m=\u001B[39m tgt_tokenizer\u001B[38;5;241m.\u001B[39mdecode(tgt_token_ids\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m), keep_specials\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[0;32m~/codes/nlp/learningtoask-final/notebooks/../src/modelutil.py:309\u001B[0m, in \u001B[0;36mSeq2Seq.greedy_generate\u001B[0;34m(self, source, source_mask, max_seq, stop_at_eos)\u001B[0m\n\u001B[1;32m    306\u001B[0m c \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandn_like(h, device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[1;32m    307\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(max_seq):\n\u001B[1;32m    308\u001B[0m     \u001B[38;5;66;03m# (1, tgt_vocab_size)\u001B[39;00m\n\u001B[0;32m--> 309\u001B[0m     logit, (h, c), score \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    310\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_out\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecoder_input\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mh\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msource_mask\u001B[49m\n\u001B[1;32m    311\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    312\u001B[0m     \u001B[38;5;66;03m# (1, 1)\u001B[39;00m\n\u001B[1;32m    313\u001B[0m     decoder_input \u001B[38;5;241m=\u001B[39m logit\u001B[38;5;241m.\u001B[39msqueeze()\u001B[38;5;241m.\u001B[39margmax(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/codes/nlp/learningtoask-final/venv/lib64/python3.10/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/codes/nlp/learningtoask-final/venv/lib64/python3.10/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/codes/nlp/learningtoask-final/notebooks/../src/modelutil.py:153\u001B[0m, in \u001B[0;36mDecoder.forward\u001B[0;34m(self, encoder_out, target, last_hidden_state, last_cell_state, source_mask)\u001B[0m\n\u001B[1;32m    151\u001B[0m concatenated \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((output, attn_based_ctx), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39msqueeze()\n\u001B[1;32m    152\u001B[0m \u001B[38;5;66;03m# => (N, vocab_size)\u001B[39;00m\n\u001B[0;32m--> 153\u001B[0m logit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoder_linear\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconcatenated\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    155\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m logit, (ht, ct), score\n",
      "File \u001B[0;32m~/codes/nlp/learningtoask-final/venv/lib64/python3.10/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/codes/nlp/learningtoask-final/venv/lib64/python3.10/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/codes/nlp/learningtoask-final/venv/lib64/python3.10/site-packages/torch/nn/modules/container.py:215\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    209\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28miter\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_modules\u001B[38;5;241m.\u001B[39mvalues())\n\u001B[1;32m    211\u001B[0m \u001B[38;5;66;03m# NB: We can't really type check this function as the type of input\u001B[39;00m\n\u001B[1;32m    212\u001B[0m \u001B[38;5;66;03m# may change dynamically (as is tested in\u001B[39;00m\n\u001B[1;32m    213\u001B[0m \u001B[38;5;66;03m# TestScript.test_sequential_intermediary_types).  Cannot annotate\u001B[39;00m\n\u001B[1;32m    214\u001B[0m \u001B[38;5;66;03m# with Any as TorchScript expects a more precise type\u001B[39;00m\n\u001B[0;32m--> 215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m    217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m module(\u001B[38;5;28minput\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(f\"{data_root}/dev.src\") as srcfile:\n",
    "    sources = srcfile.readlines()\n",
    "\n",
    "with open(f\"{data_root}/dev.tgt\") as tgtfile:\n",
    "    references = tgtfile.readlines()\n",
    "\n",
    "\n",
    "hypotheses = []\n",
    "for source in tqdm(sources):\n",
    "    hyp, _ = generate(model, source, src_tokenizer, tgt_tokenizer, cfg, method=\"greedy\")\n",
    "    hypotheses.append(hyp[0])\n",
    "\n",
    "metrics = compute_metrics(hypotheses, references)\n",
    "metrics = {k: v * 100 for k, v in metrics.items()}\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240 / 10570\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[0;32m<timed exec>:13\u001B[0m\n",
      "File \u001B[0;32m~/codes/nlp/learningtoask-final/notebooks/../src/modelutil.py:471\u001B[0m, in \u001B[0;36mgenerate\u001B[0;34m(model, sentence, src_tokenizer, tgt_tokenizer, cfg, method, p)\u001B[0m\n\u001B[1;32m    468\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tokens, attention_scores\n\u001B[1;32m    470\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m method \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnucleus\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 471\u001B[0m     tgt_token_ids, attention_scores \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnucleus_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    472\u001B[0m \u001B[43m        \u001B[49m\u001B[43msrc_token_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcfg\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtgt_max_seq\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mp\u001B[49m\n\u001B[1;32m    473\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    474\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tgt_tokenizer\u001B[38;5;241m.\u001B[39mdecode(tgt_token_ids, keep_specials\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m), attention_scores\n",
      "File \u001B[0;32m~/codes/nlp/learningtoask-final/notebooks/../src/modelutil.py:285\u001B[0m, in \u001B[0;36mSeq2Seq.nucleus_generate\u001B[0;34m(self, sources, source_masks, max_seq, p)\u001B[0m\n\u001B[1;32m    280\u001B[0m logit, (h, c), score \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder(\n\u001B[1;32m    281\u001B[0m     encoder_out, token_ids, h, c, source_masks\n\u001B[1;32m    282\u001B[0m )\n\u001B[1;32m    284\u001B[0m \u001B[38;5;66;03m# (N, 1)\u001B[39;00m\n\u001B[0;32m--> 285\u001B[0m token_ids, token_probs \u001B[38;5;241m=\u001B[39m \u001B[43mnucleus_sample\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlogit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mp\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    287\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, token_id \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(token_ids):\n\u001B[1;32m    288\u001B[0m     output_token_ids[i]\u001B[38;5;241m.\u001B[39mappend(token_id\u001B[38;5;241m.\u001B[39mitem())\n",
      "File \u001B[0;32m~/codes/nlp/learningtoask-final/notebooks/../src/modelutil.py:417\u001B[0m, in \u001B[0;36mnucleus_sample\u001B[0;34m(logits, p)\u001B[0m\n\u001B[1;32m    415\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m logits\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexpected a matrix (batch, vocab_size)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    416\u001B[0m probs \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39msoftmax(logits, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m--> 417\u001B[0m sorted_probs, sorted_indices \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msort\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprobs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdescending\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    418\u001B[0m cumulative_sum \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcumsum(sorted_probs, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    419\u001B[0m out_of_nucleus \u001B[38;5;241m=\u001B[39m cumulative_sum \u001B[38;5;241m>\u001B[39m p\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(f\"{data_root}/dev.src\") as srcfile:\n",
    "    sources = srcfile.readlines()\n",
    "\n",
    "with open(f\"{data_root}/dev.tgt\") as tgtfile:\n",
    "    references = tgtfile.readlines()\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "hypotheses = []\n",
    "\n",
    "i = 0\n",
    "while i < len(sources):\n",
    "    hyp, _ = generate(model, sources[i: i+batch_size], src_tokenizer, tgt_tokenizer, cfg, method=\"nucleus\", p=0.45)\n",
    "    hypotheses.extend(hyp)\n",
    "    i += batch_size\n",
    "    print(f\"{i} / {len(sources)}\", end=\"\\r\")\n",
    "\n",
    "metrics = compute_metrics(hypotheses, references)\n",
    "metrics = {k: v * 100 for k, v in metrics.items()}\n",
    "metrics"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T17:23:14.397895103Z",
     "start_time": "2024-05-06T17:23:05.351807751Z"
    }
   },
   "id": "151f1cb7700a8015"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "with open(f\"{data_root}/dev.src\") as srcfile:\n",
    "    sources = srcfile.readlines()\n",
    "\n",
    "with open(f\"{data_root}/dev.tgt\") as tgtfile:\n",
    "    references = tgtfile.readlines()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T17:23:14.431143405Z",
     "start_time": "2024-05-06T17:23:14.397190942Z"
    }
   },
   "id": "d0ce3f9eef79a484"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surveys of plague pit remains in france and england indicate the first variant entered europe through the port of marseille around november 1347 and spread through france over the next two years , eventually reaching england in the spring of 1349 , where it spread through the country in three epidemics . \n",
      "\n",
      "\n",
      "how and when did the first variant of y. pestis enter europe ?\n",
      "\n",
      "nucleus: when did the first peasants begin to expand europe ?\n",
      "\n",
      "beam: ['when did england begin the first ?']\n",
      "\n",
      "greedy: ['what did england use in france to the port of <UNK>']\n"
     ]
    }
   ],
   "source": [
    "# idx = random.randint(0, 10000)\n",
    "# nucleus\n",
    "hyp_nucleus, _ = generate(model, sources[idx: idx+2], src_tokenizer, tgt_tokenizer, cfg, method=\"nucleus\", p=0.7)\n",
    "\n",
    "hyp_greedy, _ = generate(model, sources[idx], src_tokenizer, tgt_tokenizer, cfg, method=\"greedy\")\n",
    "\n",
    "hyp_beam, _ = generate(model, sources[idx], src_tokenizer, tgt_tokenizer, cfg, method=\"beam\")\n",
    "\n",
    "\n",
    "print(sources[idx], references[idx], sep='\\n\\n')\n",
    "print(f\"nucleus: {hyp_nucleus[0]}\\n\\nbeam: {hyp_beam}\\n\\ngreedy: {hyp_greedy}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T17:25:49.040336450Z",
     "start_time": "2024-05-06T17:25:48.165492745Z"
    }
   },
   "id": "ebb034fa9c59b937"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "923a95c429d9562c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
