{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e2725cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jupyter_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext jupyter_black\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext jupyter_black\n",
    "\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "from src.datautil import *\n",
    "from src.modelutil import *\n",
    "from src.trainutil import *\n",
    "from src.metrics import *\n",
    "\n",
    "import yaml\n",
    "\n",
    "data_root = \"../data\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fbf6a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_dir = Path(\"./checkpoints/debug1\")\n",
    "with open(experiment_dir / \"history/config.yaml\", \"r\") as stream:\n",
    "    cfg = yaml.safe_load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afb0010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokenizer = torch.load(experiment_dir / \"src_tokenizer.pt\")\n",
    "tgt_tokenizer = torch.load(experiment_dir / \"tgt_tokenizer.pt\")\n",
    "src_vocab = src_tokenizer.vocab\n",
    "tgt_vocab = tgt_tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5bf4826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (src_embedding): Embedding(45000, 300)\n",
       "  (tgt_embedding): Embedding(28000, 300)\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(45000, 300)\n",
       "    (layers): Sequential(\n",
       "      (0): Embedding(45000, 300)\n",
       "      (1): LSTM(300, 600, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(28000, 300)\n",
       "    (lstm): LSTM(300, 600, num_layers=2, batch_first=True, dropout=0.3)\n",
       "    (attention): Attention(\n",
       "      (projection_layer): Linear(in_features=1200, out_features=600, bias=True)\n",
       "    )\n",
       "    (decoder_linear): Sequential(\n",
       "      (0): Linear(in_features=1800, out_features=600, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=600, out_features=28000, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Seq2Seq(\n",
    "    src_vocab_size=len(src_vocab),\n",
    "    tgt_vocab_size=len(tgt_vocab),\n",
    "    src_embedding_vector=None,\n",
    "    tgt_embedding_vector=None,\n",
    "    tgt_pad_index=tgt_vocab[\"<PAD>\"],\n",
    "    tgt_sos_index=tgt_vocab[\"<SOS>\"],\n",
    "    tgt_eos_index=tgt_vocab[\"<EOS>\"],\n",
    "    hidden_size=cfg[\"hidden_size\"],\n",
    "    bidirectional=cfg[\"bidirectional\"],\n",
    "    num_layers=cfg[\"num_layers\"],\n",
    "    src_embedding_size=cfg[\"src_embedding_size\"],\n",
    "    tgt_embedding_size=cfg[\"tgt_embedding_size\"],\n",
    "    dropout=cfg[\"dropout\"],\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a8a0c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _, _, epoch = load_checkpoint(model, experiment_dir / \"model_best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b25135fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 10570/10570 [03:26<00:00, 51.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 46s, sys: 889 ms, total: 3min 47s\n",
      "Wall time: 3min 49s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bleu1': 23.336224257946014,\n",
       " 'bleu2': 9.72084030508995,\n",
       " 'bleu3': 4.790161550045013,\n",
       " 'bleu4': 2.58890837430954,\n",
       " 'rougeL': 19.726860523223877}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "with open(f\"{data_root}/dev.src\") as srcfile:\n",
    "    sources = srcfile.readlines()\n",
    "\n",
    "with open(f\"{data_root}/dev.tgt\") as tgtfile:\n",
    "    references = tgtfile.readlines()\n",
    "\n",
    "hypotheses = []\n",
    "for source in tqdm(sources):\n",
    "    hyp, _ = generate(model, source, src_tokenizer, tgt_tokenizer, cfg, method=\"beam\")\n",
    "    hypotheses.append(hyp[0])\n",
    "\n",
    "metrics = compute_metrics(hypotheses, references)\n",
    "metrics = {k: v * 100 for k, v in metrics.items()}\n",
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
