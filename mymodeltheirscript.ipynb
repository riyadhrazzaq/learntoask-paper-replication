{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b387548d-6be1-4c1d-930d-5c2f78817849",
   "metadata": {},
   "source": [
    "# Experiment 2\n",
    "models are mine\n",
    "training iterations are theirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e66be32-b5ef-4f52-b619-3d97b2de2afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext jupyter_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c0fe68-697b-4cc9-a402-04e9f3332083",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/chatbot_tutorial.html?highlight=chatbot#define-evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eb5714-1d0f-47d8-af47-d3c9bcbe455e",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f318c424-4110-4854-add4-1392050a5db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import io, os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torchtext\n",
    "from torchtext.vocab import build_vocab_from_iterator, GloVe\n",
    "from torch import optim\n",
    "from nltk.translate import bleu\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec82a5f-3467-4bb2-8839-688d7c542bd0",
   "metadata": {},
   "source": [
    "# Look at Data\n",
    "Data already preprocessed collected from the original repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c00cf968-e27a-4a2f-9e94-8ea8fc5c79b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = Path(\"data/processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8e9a7cf-40f5-4e85-ab70-bcadb41a33a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70484 10570 11877\n"
     ]
    }
   ],
   "source": [
    "with open(data_root / \"src-train.txt\") as f:\n",
    "    src_train = [line.strip() for line in f]\n",
    "\n",
    "with open(data_root / \"tgt-train.txt\") as f:\n",
    "    tgt_train = [line.strip() for line in f]\n",
    "\n",
    "with open(data_root / \"src-dev.txt\") as f:\n",
    "    src_dev = [line.strip() for line in f]\n",
    "\n",
    "with open(data_root / \"tgt-dev.txt\") as f:\n",
    "    tgt_dev = [line.strip() for line in f]\n",
    "\n",
    "with open(data_root / \"src-test.txt\") as f:\n",
    "    src_test = [line.strip() for line in f]\n",
    "\n",
    "with open(data_root / \"tgt-test.txt\") as f:\n",
    "    tgt_test = [line.strip() for line in f]\n",
    "\n",
    "print(len(src_train), len(src_dev), len(src_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83e2102-7305-4c25-a1ad-7eaba2d02b5d",
   "metadata": {},
   "source": [
    "# Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcbd4847-f6b8-4268-bbda-2f02624c1fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datahandler as dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "188d7a39-b50a-4f90-a70c-aa1c7df0a73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dh.load_and_build_vocab(\n",
    "    data_root / \"src-train.txt\", data_root / \"tgt-train.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e94daf4-417b-4039-bb33-62ad2964728f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_index = vocab[\"<PAD>\"]\n",
    "sos_index = vocab[\"<SOS>\"]\n",
    "eos_index = vocab[\"<EOS>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d208acb-912a-44d4-8d96-a9ce7d2e156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a tensor of shape (vocab_size, embedding_dim)\n",
    "embedding_vector = dh.load_pretrained_glove(vocab, cache=\"data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13abfa76-f53f-48c2-ad4d-4780046e59e9",
   "metadata": {},
   "source": [
    "# Batch Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dbb44da-264e-463c-97c1-e8d4f811f8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"src_max_seq\": 40, \"tgt_max_seq\": 15, \"batch_size\": 64, \"lr\": 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a788f8c4-129e-42df-852e-42ea51b9b41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenization import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05385c8c-0967-4df9-8342-2ecfbe538739",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab, pad_index, sos_index, eos_index)\n",
    "\n",
    "src_train_tensor, src_train_mask = tokenizer.encode(\n",
    "    src_train, max_seq=config[\"src_max_seq\"]\n",
    ")\n",
    "\n",
    "tgt_train_tensor, tgt_train_mask = tokenizer.encode(\n",
    "    tgt_train, add_sos=True, add_eos=True, max_seq=config[\"tgt_max_seq\"]\n",
    ")\n",
    "\n",
    "src_test_tensor, src_test_mask = tokenizer.encode(\n",
    "    src_test, max_seq=config[\"src_max_seq\"]\n",
    ")\n",
    "tgt_test_tensor, tgt_test_mask = tokenizer.encode(\n",
    "    tgt_test, add_sos=True, add_eos=True, max_seq=config[\"tgt_max_seq\"]\n",
    ")\n",
    "\n",
    "src_dev_tensor, src_dev_mask = tokenizer.encode(src_dev, max_seq=config[\"src_max_seq\"])\n",
    "tgt_dev_tensor, tgt_dev_mask = tokenizer.encode(\n",
    "    tgt_dev, add_sos=True, add_eos=True, max_seq=config[\"tgt_max_seq\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f822241-fd5c-4548-96e4-44f2ba9de3ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a pub / <UNK> / , or public house is , despite its name , a private house , but is called a public house because it is licensed to sell alcohol to the general public .']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(src_train_tensor[0].unsqueeze(dim=0), keep_specials=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95e6fd07-1c8d-4d85-8892-cb18f80436ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a pub / pÊŒb / , or public house is , despite its name , a private house , but is called a public house because it is licensed to sell alcohol to the general public .'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34ab77b9-14a9-43a2-8a87-6861cdf35538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([70484, 40])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_train_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "199858b6-a96e-49e6-a36c-9536f2202135",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceQuestionDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sentences: torch.Tensor,\n",
    "        questions: torch.Tensor,\n",
    "        sentences_mask=None,\n",
    "        questions_mask=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Represents a dataset of text pairs for training or evaluating models that\n",
    "        deal with relationships between text passages.\n",
    "\n",
    "        Args:\n",
    "            vocab (torchtext.vocab.Vocab): A pre-built vocabulary object\n",
    "                containing word mappings from text to numerical representation.\n",
    "            sentences (List[str]): A list of text passages (sentences, paragraphs, etc.).\n",
    "            questions (List[str]): A list of corresponding questions related to the sentences.\n",
    "            Ls (int, optional): The maximum length to which sentences will be\n",
    "                truncated or padded during preprocessing (default: 150).\n",
    "            Lq (int, optional): The maximum length to which questions will be\n",
    "                truncated or padded during preprocessing (default: 50).\n",
    "        \"\"\"\n",
    "        self.sentences = sentences\n",
    "        self.questions = questions\n",
    "        self.sentences_mask = sentences_mask\n",
    "        self.questions_mask = questions_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.sentences.size(0)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            self.sentences[index],\n",
    "            self.questions[index],\n",
    "            self.sentences_mask[index],\n",
    "            self.questions_mask[index],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdd2247a-7b60-4d0c-886e-576d7e9ea4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = SentenceQuestionDataset(\n",
    "    src_train_tensor, tgt_train_tensor, src_train_mask, tgt_train_mask\n",
    ")\n",
    "test_ds = SentenceQuestionDataset(\n",
    "    src_test_tensor, tgt_train_tensor, src_test_mask, tgt_test_mask\n",
    ")\n",
    "dev_ds = SentenceQuestionDataset(\n",
    "    src_dev_tensor, tgt_dev_tensor, src_dev_mask, tgt_dev_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1cad14da-4614-4eaa-82e0-f02a15a71571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1102\n"
     ]
    }
   ],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "print(len(train_dl))\n",
    "dev_dl = DataLoader(dev_ds, batch_size=8, shuffle=False)\n",
    "test_dl = DataLoader(test_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af86b1fa-fbc3-481e-a2d4-3a0548c7eeed",
   "metadata": {},
   "source": [
    "# Model Def"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8df995-13ab-484f-a5dd-03da5e56b1f5",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f908362-9cd3-4cb8-87ba-f91990c8499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_hidden_size, decoder_hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.projection_layer = nn.Linear(encoder_hidden_size, decoder_hidden_size)\n",
    "\n",
    "    def forward(self, encoder_output, decoder_output):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            encoder_output (torch.Tensor): (N, L, encoder_hidden_size)\n",
    "            decoder_output (torch.Tensor): (N, 1, decoder_hidden_size)\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            score (torch.Tensor): (N, L, 1)\n",
    "        \"\"\"\n",
    "        # => (N, L, decoder_hidden_size)\n",
    "        projection = self.projection_layer(encoder_output)\n",
    "        # => (N, L, 1)\n",
    "        score = projection @ decoder_output.transpose(1, 2)\n",
    "        score = F.softmax(score, dim=1)\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b93460-7f8f-4b45-b45b-d2ae04f28802",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "773be5fe-d658-4cf6-ba4d-4f3d7a2510ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embedding,\n",
    "        embedding_dim,\n",
    "        hidden_dim=8,\n",
    "        bidirectional=False,\n",
    "        num_layers=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        self.encoder = nn.Sequential(\n",
    "            self.embedding,\n",
    "            nn.LSTM(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                batch_first=True,\n",
    "                bidirectional=bidirectional,\n",
    "                num_layers=num_layers,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, src: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src (torch.Tensor): (N, Ls) A batch of source sentences represented as tensors.\n",
    "        \"\"\"\n",
    "        # encoder_representation (N, Ls, d), hT => cT => (#direction * #layer, N, d) : hidden states from the last timestep\n",
    "        encoder_out, (last_hidden_state, last_cell_state) = self.encoder(src)\n",
    "        return encoder_out, last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd24d068-3cb7-48cf-bb73-a4f8c436f06d",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45fec2ea-4a71-4d8f-bde5-c4246f560217",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embedding,\n",
    "        embedding_dim,\n",
    "        hidden_dim=8,\n",
    "        encoder_bidirectional=False,\n",
    "        num_layers=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "\n",
    "        # self.attn_layer = nn.Linear(\n",
    "        #     in_features=hidden_dim * 2 if bidirectional else hidden_dim,\n",
    "        #     out_features=hidden_dim * 2 if bidirectional else hidden_dim,\n",
    "        # )\n",
    "\n",
    "        self.attention = Attention(\n",
    "            hidden_dim * 2 if encoder_bidirectional else hidden_dim, hidden_dim\n",
    "        )\n",
    "\n",
    "        self.decoder_linear = nn.Sequential(\n",
    "            # 3*hidden_dim because decoder_out and source context will be concatenated\n",
    "            # this layer is Eq 5 in the Luong et. al. paper\n",
    "            nn.Linear(\n",
    "                hidden_dim * 3 if encoder_bidirectional else hidden_dim, hidden_dim\n",
    "            ),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, vocab_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, encoder_out, target, last_hidden_state, last_cell_state):\n",
    "        x = self.embedding(target)\n",
    "        # => N, 1, d\n",
    "        output, (ht, ct) = self.lstm(x, (last_hidden_state, last_cell_state))\n",
    "        # => (N, Ls, 1)\n",
    "        score = self.attention(encoder_out, output)\n",
    "        # (N, Ls, 1) x (N, Ls, DH) => (N, Ls, DH) => (N, 1, DH)\n",
    "        # Eq 4 from the Du et. al. paper (learning to ask)\n",
    "        attn_based_ctx = (score * encoder_out).sum(dim=1).unsqueeze(dim=1)\n",
    "        # => (N, 1, d ) & (N, 1, DH)\n",
    "        concatenated = torch.cat((output, attn_based_ctx), dim=2).squeeze()\n",
    "        # => (N, vocab_size)\n",
    "        logit = self.decoder_linear(concatenated)\n",
    "\n",
    "        return logit, ht"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2271799-7035-4d9b-9821-5bb9546a4d61",
   "metadata": {},
   "source": [
    "# Training Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eaa3e4-c20d-465c-a45e-b99c89ab1ba3",
   "metadata": {},
   "source": [
    "## Single Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "baa1f527-b902-460f-b507-d6ddc82053ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    crossEntropy = torch.nn.functional.cross_entropy(inp, target, reduction=\"none\")\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eaa436a1-82b3-455f-b3a7-1770269a0353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    input_variable,\n",
    "    lengths,\n",
    "    target_variable,\n",
    "    mask,\n",
    "    max_target_len,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    embedding,\n",
    "    encoder_optimizer,\n",
    "    decoder_optimizer,\n",
    "    batch_size,\n",
    "    clip,\n",
    "):\n",
    "\n",
    "    # Zero gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Set device options\n",
    "    input_variable = input_variable.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    # Lengths for RNN packing should always be on the CPU\n",
    "    lengths = lengths.to(\"cpu\")\n",
    "\n",
    "    # Initialize variables\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    # Forward pass through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable)\n",
    "\n",
    "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
    "    decoder_input = torch.LongTensor(\n",
    "        [[sos_index for _ in range(batch_size)]]\n",
    "    ).transpose(0, 1)\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    # Set initial decoder hidden state to the encoder's final hidden state\n",
    "    decoder_hidden = encoder_hidden[:num_layers]\n",
    "\n",
    "    # Forward batch of sequences through decoder one time step at a time\n",
    "    for t in range(max_target_len):\n",
    "        decoder_output, decoder_hidden = decoder(\n",
    "            encoder_outputs,\n",
    "            decoder_input,\n",
    "            decoder_hidden,\n",
    "            torch.randn_like(decoder_hidden),\n",
    "        )\n",
    "        # Teacher forcing: next input is current target\n",
    "        decoder_input = target_variable[:, t].view(-1, 1)\n",
    "        # Calculate and accumulate loss\n",
    "        mask_loss, nTotal = maskNLLLoss(\n",
    "            decoder_output, target_variable[:, t], mask[:, t]\n",
    "        )\n",
    "        loss += mask_loss\n",
    "        print_losses.append(mask_loss.item() * nTotal)\n",
    "        n_totals += nTotal\n",
    "\n",
    "    # Perform backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients: gradients are modified in place\n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # Adjust model weights\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cdd8b9-6a75-4b67-bebc-c8bc30da5ed0",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "524acc39-da4b-4a09-860b-76848802f33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(\n",
    "    model_name,\n",
    "    vocab,\n",
    "    train_dl,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    encoder_optimizer,\n",
    "    decoder_optimizer,\n",
    "    embedding,\n",
    "    encoder_n_layers,\n",
    "    decoder_n_layers,\n",
    "    save_dir,\n",
    "    n_iteration,\n",
    "    batch_size,\n",
    "    print_every,\n",
    "    save_every,\n",
    "    clip,\n",
    "    corpus_name,\n",
    "    loadFilename,\n",
    "):\n",
    "\n",
    "    # Initializations\n",
    "    print(\"Initializing ...\")\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    if loadFilename:\n",
    "        start_iteration = checkpoint[\"iteration\"] + 1\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Training...\")\n",
    "    for iteration, training_batch in enumerate(train_dl):\n",
    "        # Extract fields from batch\n",
    "        input_variable, target_variable, input_mask, target_mask = training_batch\n",
    "        lengths = input_mask.sum(dim=1)\n",
    "\n",
    "        # Run a training iteration with batch\n",
    "        loss = train(\n",
    "            input_variable,\n",
    "            lengths,\n",
    "            target_variable,\n",
    "            target_mask,\n",
    "            config[\"tgt_max_seq\"],\n",
    "            encoder,\n",
    "            decoder,\n",
    "            embedding,\n",
    "            encoder_optimizer,\n",
    "            decoder_optimizer,\n",
    "            batch_size,\n",
    "            clip,\n",
    "        )\n",
    "        print_loss += loss\n",
    "\n",
    "        # Print progress\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            print(\n",
    "                \"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(\n",
    "                    iteration, iteration / n_iteration * 100, print_loss_avg\n",
    "                )\n",
    "            )\n",
    "            print_loss = 0\n",
    "\n",
    "        # Save checkpoint\n",
    "        if iteration % save_every == 0:\n",
    "            directory = os.path.join(\n",
    "                save_dir,\n",
    "                model_name,\n",
    "                corpus_name,\n",
    "                \"{}-{}_{}\".format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
    "            )\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"iteration\": iteration,\n",
    "                    \"en\": encoder.state_dict(),\n",
    "                    \"de\": decoder.state_dict(),\n",
    "                    \"en_opt\": encoder_optimizer.state_dict(),\n",
    "                    \"de_opt\": decoder_optimizer.state_dict(),\n",
    "                    \"loss\": loss,\n",
    "                    \"voc_dict\": vocab.get_stoi(),\n",
    "                    \"embedding\": embedding.state_dict(),\n",
    "                },\n",
    "                os.path.join(directory, \"{}_{}.tar\".format(iteration, \"checkpoint\")),\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0fe0d3-219e-4c2c-80f1-795fe55dd17b",
   "metadata": {},
   "source": [
    "# Train Now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13704c1f-8800-40b9-acee-b01ed2eb359d",
   "metadata": {},
   "source": [
    "## Model Instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "16bfaaae-88a2-4c3b-8a6d-2132c62fe1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 300\n",
    "bidirectional = True\n",
    "num_layers = 2\n",
    "hidden_dim = 500\n",
    "hidden_size = hidden_dim\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "47671b2a-83e0-4df3-9e77-42162676e317",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "encoder = Encoder(\n",
    "    vocab_size,\n",
    "    embedding,\n",
    "    embedding_dim,\n",
    "    hidden_dim,\n",
    "    bidirectional,\n",
    "    num_layers,\n",
    ")\n",
    "decoder = Decoder(\n",
    "    vocab_size,\n",
    "    embedding,\n",
    "    embedding_dim,\n",
    "    hidden_dim,\n",
    "    bidirectional,\n",
    "    num_layers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8a2dee-d1d1-48d0-9081-a042363a1175",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "de4ab065-3897-4233-8768-bada0061da9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building optimizers ...\n",
      "Starting Training!\n",
      "Initializing ...\n",
      "Training...\n",
      "Iteration: 0; Percent complete: 0.0%; Average loss: 10.8124\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Run training iterations\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting Training!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m \u001b[43mtrainIters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m           \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m           \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_every\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloadFilename\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[34], line 38\u001b[0m, in \u001b[0;36mtrainIters\u001b[0;34m(model_name, vocab, train_dl, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename)\u001b[0m\n\u001b[1;32m     34\u001b[0m lengths \u001b[38;5;241m=\u001b[39m input_mask\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Run a training iteration with batch\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_variable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_variable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtgt_max_seq\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m print_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Print progress\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[36], line 64\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\u001b[0m\n\u001b[1;32m     61\u001b[0m     n_totals \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m nTotal\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Perform backpropagation\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Clip gradients: gradients are modified in place\u001b[39;00m\n\u001b[1;32m     67\u001b[0m _ \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(encoder\u001b[38;5;241m.\u001b[39mparameters(), clip)\n",
      "File \u001b[0;32m~/codes/nlp/learn2ask/venv/lib64/python3.9/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/codes/nlp/learn2ask/venv/lib64/python3.9/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Configure training/optimization\n",
    "model_name = 'cb_model'\n",
    "corpus_name='squad'\n",
    "\n",
    "# Set checkpoint to load from; set to None if starting from scratch\n",
    "loadFilename = None\n",
    "checkpoint_iter = 4000\n",
    "save_dir = './save'\n",
    "\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 4000\n",
    "print_every = 1\n",
    "save_every = 500\n",
    "\n",
    "# Ensure dropout layers are in train mode\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# Initialize optimizers\n",
    "print('Building optimizers ...')\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "\n",
    "# If you have CUDA, configure CUDA to call\n",
    "for state in encoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "for state in decoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "# Run training iterations\n",
    "print(\"Starting Training!\")\n",
    "trainIters(model_name, vocab, train_dl, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding, num_layers, num_layers, save_dir, n_iteration, batch_size,\n",
    "           print_every, save_every, clip, corpus_name, loadFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4ebaf8-838a-4c4c-80de-57279d1715ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
